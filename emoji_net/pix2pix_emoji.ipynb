{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow.keras as keras\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU, UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from tensorflow import logging\n",
    "import imageio, skimage\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import datetime, os, pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_filepath,target_size):\n",
    "    _, _, n_C = target_size\n",
    "    if n_C == 1:\n",
    "        mode='L'\n",
    "    elif n_C == 3:\n",
    "        mode='RGB'\n",
    "    else:\n",
    "        raise Exception('Unexpected number of chanel '+str(n_C)+'!')\n",
    "    # x = imageio.imread(img_filepath,as_gray=as_gray).astype(np.float)\n",
    "    x = scipy.misc.imread(img_filepath, mode=mode).astype(np.float)\n",
    "    x = scipy.misc.imresize(x, target_size)\n",
    "    if n_C == 1 :\n",
    "        x = np.stack((x,)*1, -1)\n",
    "    x = np.array(x)/127.5 - 1.\n",
    "    return x\n",
    "\n",
    "def load_images(target_size):\n",
    "    n_H, n_W, n_C = input_shape\n",
    "    data_dir='data/raw/'\n",
    "    img_files = [f for f in listdir(data_dir) if isfile(join(data_dir, f)) and '_x.jpg' in f]\n",
    "\n",
    "    n_x=len(img_files)\n",
    "    n_x=10\n",
    "    X=np.zeros((n_x,n_H, n_W, n_C))\n",
    "    Y=np.zeros((n_x,n_H, n_W, n_C))\n",
    "    for i in range(n_x):\n",
    "        img_id=img_files[i].strip('_x.jpg').strip('data_')\n",
    "        X[i,:,:,:] = get_img(data_dir+'data_'+str(img_id)+'_x.jpg',target_size)\n",
    "        Y[i,:,:,:] = get_img(data_dir+'data_'+str(img_id)+'_y.jpg',target_size)\n",
    "    return X,Y\n",
    "\n",
    "def load_facades_images(input_shape):\n",
    "    n_H, n_W, n_C = input_shape\n",
    "    test_data='data/facades/train/'\n",
    "    img_files = [f for f in listdir(test_data) if isfile(join(test_data, f)) and '.jpg' in f]\n",
    "    \n",
    "    n_x=len(img_files)\n",
    "    n_x=10\n",
    "    X=np.zeros((n_x,n_H, n_W, n_C))\n",
    "    Y=np.zeros((n_x,n_H, n_W, n_C))\n",
    "    for i in range(n_x):\n",
    "        img = get_img(test_data+img_files[i],[n_H,n_W*2,n_C])\n",
    "        Y[i,:,:,:], X[i,:,:,:] = img[:, :n_W, :], img[:, n_W:, :]\n",
    "    return X,Y\n",
    "\n",
    "def load_realworld_images(input_shape):\n",
    "    test_data_dir='data/test/'\n",
    "    img_training_files = [f for f in listdir(test_data_dir) if isfile(join(test_data_dir, f)) and '.jpg' in f]\n",
    "    X_test=np.zeros((len(img_training_files),target_size[0], target_size[1], target_size[2]))\n",
    "    for i in range(len(img_training_files)):\n",
    "        X_test[i,:,:,:] = get_img(test_data_dir+img_training_files[i],target_size)\n",
    "    return X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/eriklindernoren/Keras-GAN\n",
    "\n",
    "def build_generator(input_shape,gf,name):\n",
    "    \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "    n_H, n_W, n_C = input_shape\n",
    "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        u = UpSampling2D(size=2)(layer_input)\n",
    "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "        if dropout_rate:\n",
    "            u = Dropout(dropout_rate)(u)\n",
    "        u = BatchNormalization(momentum=0.8)(u)\n",
    "        u = Concatenate()([u, skip_input])\n",
    "        return u\n",
    "\n",
    "    # Image input\n",
    "    d0 = Input(shape=input_shape)\n",
    "\n",
    "    # Downsampling\n",
    "    d1 = conv2d(d0, gf, bn=False)\n",
    "    d2 = conv2d(d1, gf*2)\n",
    "    d3 = conv2d(d2, gf*4)\n",
    "    d4 = conv2d(d3, gf*8)\n",
    "    d5 = conv2d(d4, gf*8)\n",
    "    d6 = conv2d(d5, gf*8)\n",
    "    d7 = conv2d(d6, gf*8)\n",
    "\n",
    "    # Upsampling\n",
    "    u1 = deconv2d(d7, d6, gf*8)\n",
    "    u2 = deconv2d(u1, d5, gf*8)\n",
    "    u3 = deconv2d(u2, d4, gf*8)\n",
    "    u4 = deconv2d(u3, d3, gf*4)\n",
    "    u5 = deconv2d(u4, d2, gf*2)\n",
    "    u6 = deconv2d(u5, d1, gf)\n",
    "\n",
    "    u7 = UpSampling2D(size=2)(u6)\n",
    "    output_img = Conv2D(n_C, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "    return Model(d0, output_img,name=name)\n",
    "\n",
    "def build_discriminator(input_shape, df, name):\n",
    "    \n",
    "    n_H, n_W, n_C = input_shape\n",
    "    def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        if bn:\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "        return d\n",
    "\n",
    "    img_A = Input(shape=input_shape)\n",
    "    img_B = Input(shape=input_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image by channels to produce input\n",
    "    combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "    d1 = d_layer(combined_imgs, df, bn=False)\n",
    "    d2 = d_layer(d1, df*2)\n",
    "    d3 = d_layer(d2, df*4)\n",
    "    d4 = d_layer(d3, df*8)\n",
    "\n",
    "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "    return Model([img_A, img_B], validity, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, gf=64, df=64, name='combined',init_model=True):    \n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_discriminator(input_shape,df,'discriminator')\n",
    "    discriminator.compile(loss='mse',optimizer=Adam(0.0002, 0.5),metrics=['accuracy'])\n",
    "\n",
    "    #-------------------------\n",
    "    # Build the generator\n",
    "    generator = build_generator(input_shape,gf,'generator')\n",
    "\n",
    "    # Input images and their conditioning images\n",
    "    img_A = Input(shape=input_shape)\n",
    "    img_B = Input(shape=input_shape)\n",
    "\n",
    "    # By conditioning on B generate a fake version of A\n",
    "    fake_A = generator(img_B)\n",
    "\n",
    "    # For the combined model we will only train the generator\n",
    "    \"\"\"By setting trainable=False after the discriminator has been compiled the discriminator \n",
    "    is still trained during discriminator.train_on_batch but since it's set to non-trainable \n",
    "    before the combined model is compiled it's not trained during combined.train_on_batch.\"\"\"\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # Discriminators determines validity of translated images / condition pairs\n",
    "    valid = discriminator([fake_A, img_B])\n",
    "\n",
    "    combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A], name=name)\n",
    "    combined.compile(loss=['mse', 'mae'],loss_weights=[1, 100],optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "    return generator, discriminator, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(generator, imgs_X, imgs_Y,epoch):\n",
    "    m, n_H, n_W, _ = imgs_X.shape\n",
    "    DPI = plt.gcf().get_dpi()\n",
    "    figsize=((3*n_W)/float(DPI),(m*n_H)/float(DPI))\n",
    "    \n",
    "    generated_Y = generator.predict(imgs_X,batch_size=1)\n",
    "    titles = ['Original', 'Generated', 'Condition']\n",
    "    fig, axs = plt.subplots(m, len(titles),figsize=figsize)\n",
    "    \n",
    "    for r in range(m):\n",
    "        axs[r,0].imshow(0.5 * imgs_X[r,:,:,:]+ 0.5)\n",
    "        axs[r,0].set_title(titles[0])\n",
    "        axs[r,0].axis('off')\n",
    "        axs[r,1].imshow(0.5 * generated_Y[r,:,:,:]+ 0.5)\n",
    "        axs[r,1].set_title(titles[1])\n",
    "        axs[r,1].axis('off')\n",
    "        axs[r,2].imshow(0.5 * imgs_Y[r,:,:,:]+ 0.5)\n",
    "        axs[r,2].set_title(titles[2])\n",
    "        axs[r,2].axis('off')\n",
    "    fig.savefig(\"output/pix2pix_epoch_%d.png\" % (epoch))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(generator, discriminator, combined,imgs_A, imgs_B, epochs=1, batch_size=1):    \n",
    "    # Calculate output shape of D (PatchGAN)\n",
    "    m, n_H, n_W, n_C = imgs_B.shape \n",
    "    disc_patch = (int(n_H/16), int(n_W/16), 1)\n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((m,) + disc_patch)\n",
    "    fake = np.zeros((m,) + disc_patch)\n",
    "\n",
    "    logging.info('Training Discriminator')\n",
    "    # Condition on B and generate a translated version\n",
    "    fake_A = generator.predict(imgs_B,batch_size=batch_size)\n",
    "    # Train the discriminators (original images = real / generated = Fake)\n",
    "    d_loss_real = discriminator.fit(x=[imgs_A, imgs_B], y=valid, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    d_loss_fake = discriminator.fit(x=[fake_A, imgs_B], y=fake, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    d_loss = 0.5 * np.add(d_loss_real.history['loss'], d_loss_fake.history['loss'])\n",
    "\n",
    "    logging.info('Training Generator')\n",
    "    # Train the generators. SET Discriminator trainable false.\n",
    "    g_loss = combined.fit(x=[imgs_A, imgs_B], y=[valid, imgs_A], batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    loss={'d_loss_real':d_loss_real,'d_loss_fake':d_loss_fake,'g_loss':g_loss}\n",
    "    return generator, discriminator, combined, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=[256,256,3]\n",
    "X,Y = load_images(input_shape)\n",
    "#X,Y = load_facades_images(input_shape)\n",
    "\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X,Y,test_size=0.1,random_state=2)\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X,Y,test_size=2,random_state=2)\n",
    "if True:\n",
    "    i_sample=np.random.randint(len(X))\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "    ax1.imshow(0.5 * X[i_sample,:,:,:] + 0.5)\n",
    "    ax2.imshow(0.5 * Y[i_sample,:,:,:] + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "generator, discriminator, combined = build_model(input_shape,init_model=True)\n",
    "for epoch in range(0, 200):\n",
    "    \n",
    "    generator, discriminator, combined ,_=train_epoch(generator, discriminator, combined,imgs_A=Y, imgs_B=X, epochs=1, batch_size=16)\n",
    "    \n",
    "    logging.info('saving model')\n",
    "    pickle.dump(combined,open('saved_model/pix2pix_emoji_combined.pkl',\"wb\" ))\n",
    "    if epoch % 5 == 0 : \n",
    "        pickle.dump(combined,open('saved_model/pix2pix_emoji_combined_epoch'+str(epoch)+'.pkl',\"wb\" ))\n",
    "    \n",
    "    # generate sample images from dataset\n",
    "    np.random.seed(3)\n",
    "    m = X.shape[0]\n",
    "    _s=np.random.randint(m-5)\n",
    "    X_sample, Y_sample =X[_s:_s+5,:,:,:], Y[_s:_s+5,:,:,:]\n",
    "    sample_images(generator,X_sample, Y_sample,epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
